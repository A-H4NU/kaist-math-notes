\documentclass[MAS212_Note.tex]{subfiles}
\begin{document}

\section{Eigenvalues}

\dfn{Eigenvalue}{
    Let \(V\) be a vector space over \(F\).
    Let \(T \colon V \to V\) be a linear operator.
    \begin{itemize}[nolistsep]
        \ii \(c \in F\) is said to be an \textit{eigenvalue} (or a \textit{characteristic value}) of \(T\)
            if there exists \(v \in V \setminus \{0\}\) such that \(T(v) = cv\).
            Such \(v\) is called an \textit{eigenvector} (or a \textit{characteristic vector})
            of \(T\) associated to \(c\).

        \ii For each \(c \in F\), \(E_c \triangleq \{\,v \in V \mid T(v) = cv\,\}\)
            is called an \textit{eigenspace} (or a \textit{characteristic space})
            associated to \(c\).
    \end{itemize}
}

\thm[eigenTFAE]{}{
    Let \(V\) be a vector space over \(F\).
    Let \(T \colon V \to V\) be a linear operator.
    Then, \textsf{TFAE}.
    \begin{enumerate}[nolistsep, label=(\roman*)]
        \ii \(c \in F\) is an eigenvalue of \(T\).
        \ii \(T - cI\) is singular.
        \ii \(\det (T - cI) = 0\).
    \end{enumerate}
}
\pf{Proof}{
    The equivalence of (i) and (ii) is trivial.
    The equivalence of (ii) and (iii) is evident from \Cref{cor:AadjA}.
}

\dfn{Characteristic Polynomial}{
    Let \(A\) be an \(n \times n\) matrix over \(F\).
    Define \(f(x) \triangleq \det (xI - A) \in F[x]\).
    Then, \(f\) is a monic polynomial in \(x\) of degree \(n = \dim V\).

    If there exists a basis \(\mcal B\) for \(V\) and \(A = [T]_{\mcal B}\),
    then we call \(f(x) = \det (xI - A)\) the \textit{characteristic polynomial} of \(T\).
}
\nt{
    \noindent
    The choice of basis does not affect the characteristic polynomial.
    See \Cref{th:diffBasisSimilar}.
}

\nt{
    \noindent
    If \(f\) is a characteristic polynomial of \(T\),
    then \(f(c) = 0\) if and only if \(c\) is an eigenvalue of \(T\).
}

\cor{}{
    If \(T\) is a linear operator on \(V\),
    then there are at most \(n\) eigenvalues of \(T\).
}
\pf{Proof}{
    Every polynomial of degree \(n\) has at most \(n\) solutions.
}

\dfn{Diagonalizability}{
    Let \(V\) be a finite-dimensional vector space over \(F\).
    Let \(T \in L(V)\).
    We say \(T\) is \textit{diagonalizable}
    if there exists a basis \(\mcal B\) such that it consists of eigenvectors of \(T\).
}

\nt{
    \begin{itemize}[nolistsep]
        \ii If \(\mcal B = \{\,v_1, \cdots, v_n\,\}\) and
            \(Tv_i = c_iv_i\) for each \(i \in [n]\), then
            \([T]_{\mcal B} = \mrm{diag}(c_1, c_2, \cdots, c_n)\).
        \ii If \(T \in L(V)\) is diagonalizable, then the characteristic polynomial
            can be completely decomposed into a product of linear factors.
    \end{itemize}
}

\mlemma[eigenspaceIndep]{}{
    Let \(V\) be a finite-dimensional vector space over \(F\).
    Let \(T \in L(V)\).
    Suppose \(c_1, \cdots, c_k \in F\) are all the possible distinct
    characteristic values of \(T\).
    Let \(W_i\) be the eigenspace of \(c_i\), i.e., \(W_i = \ker (T - c_i I)\).
    Then, if \(\mcal B_i\) is a basis for \(W_i\) for each \(i \in [k]\),
    \(\bigcup_{i=1}^k \mcal B_i\) is a basis for \(\sum_{i=1}^{k} W_i\).
}
\pf{Proof}{
    Suppose \(\sum \beta_i = 0\) where \(\beta_i \in W_i\).
    Then, applying \(T, T^2, \cdots, T^{k-1}\), we get
    \[
        \textstyle \sum_{i=1}^k c_i^j \beta_i = 0
    \]
    for each \(j \in \{0,\cdots,k-1\}\). As \[
        \begin{bmatrix}
            1 & 1 & \cdots & 1 \\
            c_1 & c_2 & \cdots & c_k \\
            \vdots & \vdots & \ddots & \vdots \\
            c_1^{k-1} & c_2^{k-1} & \cdots & c_k^{k-1}
        \end{bmatrix}
    \]
    is invertible since \(c_i\)'s are distinct, we get \(\beta_i = 0\) for each \(i\).
}

\nt{
    \Cref{lem:eigenspaceIndep} also implies that
    \(\dim \big(\sum_{i=1}^{k} W_i\big) = \sum_{i=1}^k \dim W_i\).
}

\thm[diagTFAE]{}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(T \in L(V)\).
    Suppose \(c_1, \cdots, c_k \in F\) are all the possible distinct
    characteristic values of \(T\).
    Let \(W_i\) be the eigenspace of \(c_i\), i.e., \(W_i = \ker (T - c_i I)\).
    \textsf{TFAE}.
    \begin{enumerate}[nolistsep, label=(\roman*)]
        \ii \(T\) is diagonalizable.
        \ii The characteristic polynomial is \(p(x) = \prod_{i=1}^k (x-c_i)^{d_i}\)
            where \(d_i = \dim W_i\).
        \ii \(\sum_{i=1}^k d_i = n\).
    \end{enumerate}
}
\pf{Proof}{
    (\(\text{(i)} \Rightarrow \text{(ii)}\))
    Let \(\mcal B\) be the basis for \(V\) that consists of eigenvectors of \(T\).
    If \(\mcal B_i\) is the part of \(\mcal B\) that only consists of eigenvectors
    corresponding to \(c_i\), we have \(\spn B_i = W_i\).
    Hence, on rearranging, \([T]_{\mcal B} = \mrm{diag}(\overbrace{c_1, \cdots, c_1}^{d_1},
    \overbrace{c_2, \cdots, c_2}^{d_2}, \cdots, \overbrace{c_k, \cdots, c_k}^{d_k})\).

    (\(\text{(ii)} \Rightarrow \text{(iii)}\))
    A direct consequence of \Cref{lem:eigenspaceIndep}.

    (\(\text{(iii)} \Rightarrow \text{(i)}\))
    \(\dim \sum W_i = \sum \dim W_i = \sum d_i = n\). Hence, \(\sum W_i = V\),
    i.e., \(V\) has a basis consisting of characteristic vectors.
}

\section{Annihilating Polynomials}

\nt{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(T \in L(V)\).
    \(\{\,f \in F[x] \mid f(T) = 0\,\}\) is a nonzero ideal
    as \(\{\,I, T, T^2, \cdots, T^{n^2}\,\}\) is linearly dependent.
}

\dfn{Minimal Polynomial}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(T \in L(V)\).
    The monic generator of the nonzero ideal \(\{\,f \in F[x] \mid f(T) = 0\,\}\)
    is called the \textit{minimal polynomial} of \(T\).
}

\thm[charAndMinSameSol]{}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(T \in L(V)\).
    If \(p(x)\) is the characteristic polynomial of \(T\) and \(m(x)\) is the minimal polynomial of \(T\),
    then \(p(x)\) and \(m(x)\) has the same solutions in \(F\).
}
\pf{Proof}{
    (\(\Rightarrow\))
    Suppose \(m(c) = 0\). Then, \(m(x) = (x-c)q(x)\) for some \(q \in F[x]\).
    As \(m\) is minimal, \(q(T) \neq 0\).
    This means that \(q(T)(\beta) \neq 0\) for some \(\beta \in V\).
    However, \(m(T)(\beta) = ((T-cI)q(T))(\beta) = 0\); hence \(q(T)(\beta) \in \ker (T - cI)\),
    i.e., \(c\) is an eigenvalue. This means that \(p(c) = 0\).

    (\(\Leftarrow\))
    Suppose \(p(c) = 0\), i.e., \(T(\alpha) = c \alpha\) for some nonzero \(\alpha \in V\).
    As \(T^{k}(\alpha) = c^k \alpha\) for all \(k \in \NN \cup \{0\}\),
    for any polynomial \(f \in F[x]\), we have \(f(T)(\alpha) = f(c) \alpha\).
    In particular, \(0 = m(T) \alpha = m(c) \alpha\), i.e., \(m(c) = 0\).
}

\cor{}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(T \in L(V)\).
    Suppose \(c_1, \cdots, c_k \in F\) are all the possible distinct
    characteristic values of \(T\).
    If \(p(x)\) is the characteristic polynomial of \(T\) and \(m(x)\) is the minimal polynomial of \(T\),
    then, \(p(x) = \prod_{i=1}^k (x-c_i)^{d_i}\) and \(p(x) = \prod_{i=1}^k (x-c_i)^{r_i}\)
    where \(d_i \ge r_i\) for each \(i \in [k]\).
}

\thm[cayley]{Cayley-Hamilton}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(T \in L(V)\).
    If \(p(x)\) is the characteristic polynomial of \(T\),
    then \(p(T) = 0\).
}
\pf{Proof}{
    Let \(K \triangleq \{\,h(T) \mid h \in F[x]\,\}\) be a commutative ring.
    Let \(\mcal B = \{\alpha_1, \cdots, \alpha_n\}\)
    be a basis for \(V\).
    Let \(A \triangleq [T]_{\mcal B}\) so that \(T(\alpha_i) = \sum_{j=1}^{n} A_{ji} \alpha_j\).
    This is equivalent to \(\sum_{j=1}^{n} (\delta_{ij}T - A_{ji}I) \alpha_j = 0\).

    Let \(B_{ij} \triangleq \delta_{ij}T - A_{ji}I \in K\)
    and \(B \triangleq [B_{ij}]\).
    Then, \((\adj B) B = B (\adj B) = (\det B) I\).
    By construction,
    \(\sum_{j=1}^{n} (\adj B)_{ki} B_{ij} \alpha_j = 0\)
    for all \(k, i \in [n]\).

    Taking sum over \(i\), we have
    \[\begin{aligned}[t]
        0 &= \sum_{i=1}^{n} \sum_{j=1}^{n} (\adj B)_{ki} B_{ij} \alpha_j \\
          &= \sum_{j=1}^{n} \left( \sum_{i=1}^{n} (\adj B)_{ki} B_{ij} \right) \alpha_j \\
          &= \sum_{j=1}^{n} \delta_{kj} (\det B) \alpha_j = (\det B) \alpha_k
    \end{aligned}\]
    for each \(k \in [n]\).
    As \(\{\alpha_1, \cdots, \alpha_n\}\) is a basis of \(V\),
    we have \(\det B = 0\), i.e., \(p(T) = 0\).
}

\section{Invariant Subspaces}

\dfn{\(T\)-Invariant Subspace}{
    Let \(V\) be a finite-dimensional vector space over \(F\)
    and \(W\) be a subspace of \(V\).
    Let \(T \in L(V))\).
    Then, \(W\) is said to be a \textit{\(T\)-invariant subspace}
    if \(T(W) \subseteq W\).
}
\nt{
    \noindent
    If \(W\) is a \(T\)-invariant subspace of \(V\),
    then \(T\big|_W\) is a naturally induced linear operator on \(W\).
}

\exmp{}{
    \noindent
    Let \(V\) be a finite-dimensional vector space over \(F\)
    and \(T \in L(V)\).
    \begin{itemize}[nolistsep]
        \ii \(W = \{0\}\) is a \(T\)-invariant subspace.
        \ii For every eigenvalue \(c\) of \(T\), \(E_c = \ker (T-cI)\) is a \(T\)-invariant subspace.
    \end{itemize}
}

\mlemma[invariantMinAndChar]{}{
    Let \(V\) be a finite-dimensional vector space over \(F\)
    and \(T \in L(V)\).
    Let \(W\) be a \(T\)-invariant subspace of \(V\).
    Then, \(m_W \mid m\) and \(f_W \mid f\)
    where \(m_W\) and \(m\) are minimal polynomials of \(T\big|_W\) and \(T\),
    and \(f_W\) and \(f\) are characteristic polynomials of \(T\big|_W\) and \(T\).
}
\pf{Proof}{
    Let \(\mcal B' = \{\,\alpha_1, \cdots, \alpha_k\,\}\) be a basis for \(W\),
    and extend it to \(\mcal B = \{\,\alpha_1, \cdots, \alpha_n\,\}\) so
    \(\mcal B\) is a basis for \(V\).
    As \(W\) is \(T\)-invariant, we have
    \[
        M \triangleq [T]_{\mcal B} = \begin{bmatrix}
            A & B \\ 0 & C
        \end{bmatrix}
    \]
    where \(A = \left[T\big|_W\right]_{\mcal B'}\).
    Hence, \(f(x) = \det (xI - M) = \det (xI - A) \det (xI-C)
    = f_W(x) \det(xI-C)\).

    Now, noting that \(M^r = \left[\begin{smallmatrix} A^r & \ast \\ 0 & C^r \end{smallmatrix}\right]\),
    whenever \(p(x) \in F[x]\) satisfies \(p(M) = 0\),
    we always have \(p(A) = 0\) as \(A\) is invertible; \(m(A) = 0\).
    By the definition of \(m_W\), we have \(m_W \mid m\).
}

\dfn{\(T\)-conductor}{
    Let \(V\) be a finite-dimensional vector space over \(F\)
    and \(T \in L(V)\).
    Let \(W\) be a \(T\)-invariant subspace of \(V\).
    Then, for each \(\alpha \in V\), the set
    \[
        S_T(\alpha; W) \triangleq \{\,g \in F[x] \mid g(T)\alpha \in W\,\}
    \]
    is called the \textit{\(T\)-conductor of \(\alpha\) to \(W\)}.
}

\mlemma[conductorNonzeroIdeal]{}{
    Let \(V\) be a finite-dimensional vector space over \(F\)
    and \(T \in L(V)\).
    Let \(W\) be a \(T\)-invariant subspace of \(V\).
    Then, for each \(\alpha \in V\), \(S_T(\alpha; W)\) is a
    nonzero ideal.
}
\pf{Proof}{
    \(S_T(\alpha, W)\) is nonzero as the characteristic polynomial
    is contained in the set by \Cref{th:cayley}.

    It is evident that \(S_T(\alpha, W)\) is a subspace of \(F[x]\).
    Now, take any \(h \in F[x]\) and \(g \in S_T(\alpha; W)\).
    Then, \((hg)(T)\alpha = h(T) g(T) \alpha \in W\) as \(W\) is \(T\)-invariant and
    \(g(T)\alpha \in W\).
}

\dfn{\(T\)-conductor}{
    Due to \Cref{lem:conductorNonzeroIdeal} and \Cref{th:polyIdealIsPrincipal},
    there uniquely exists the monic generator \(g_{T,\alpha,W}\) of \(S_T(\alpha, W)\).
    \(g_{T,\alpha,W}\) is also often called the \textit{\(T\)-conductor of \(\alpha\) to \(W\)}.
}

\nt{
    Since \(m(T) = f(T) = 0\) where \(m\) and \(f\) are minimal and characteristic polynomials
    of \(T\), they are elements of \(S_T(\alpha, W)\) for any \(\alpha, W\).
    Hence, \[
        g_{T,\alpha,W} \mid m \mid f\text{.}
    \]
}

\dfn{Triangulable Matrix}{
    Let \(V\) be a finite-dimensional vector space over \(F\)
    and \(T \in L(V)\).
    \(T\) is said to be \textit{triangulable} if there exists basis \(\mcal B\) for \(V\)
    such that \([T]_{\mcal B}\) is upper triangular matrix.
}

\nt{
    \noindent
    If \(T\) is diagonalizable, then \(T\) is triangulable.
}

\mlemma[TCondIsLinear]{}{
    Let \(V\) be a finite-dimensional vector space over \(F\).
    Let \(T \colon V \to V\) be a linear operator on \(V\)
    such that the minimal polynomial \(m\) of \(T\) has the form of
    \[
        m(x) = \textstyle \prod_{i=1}^k (x-c_i)^{r_i}.
    \]
    If \(W\) is a proper subspace of \(V\), then there exists \(\alpha \in V \setminus W\) and
    an eigenvalue \(c \in F\) such that \((T-cI)\alpha \in W\).
    In other words, \(x-c\) is the \(T\)-conductor of \(\alpha\) on \(W\).
}
\pf{Proof}{
    Take \(\beta \in V \setminus W\). Then, \(g \triangleq g_{T,\beta,W} \mid m\), i.e.,
    \[
        g(x) = \textstyle \prod_{i=1}^k (x-c_i)^{e_i}.
    \]
    By the definition of \(g\), and since \(\beta \notin W\),
    there exists \(j \in [k]\) such that \(e_j \ge 1\).
    \(g(x) = (x-c_j) h(x)\) for some \(h \in F[x]\).
    By the minimality of \(g\), \(\alpha \triangleq h(T)\beta \notin V \setminus W\)
    but  \((T-c_jI)\alpha = (T-c_jI)h(T)\beta = g(T) \beta \in W\).
}

\nt{
    \noindent
    For \(\alpha \notin W\) and \(T \in L(V)\), \textsf{TFAE}.
    \begin{enumerate}[nolistsep, label=(\roman*)]
        \ii \((T-cI) \alpha \in W\) for some \(c \in F\).
        \ii \(x-c\) is the \(T\)-conductor of \(\alpha\) on \(W\) for some \(c \in F\).
        \ii \(T \alpha \in \spn \{\,W, \alpha\,\}\).
    \end{enumerate}
}

\thm[triIffMin]{}{
    Let \(V\) be a finite-dimensional vector space over \(F\).
    Let \(T \colon V \to V\) be a linear operator on \(V\).
    Then, \(T\) is triangulable if and only if
    the minimal polynomial of \(T\) is a product of linear polynomials over \(F\).
}
\pf{Proof}{
    (\(\Rightarrow\)) Since \(T\) is triangulable, there exists a basis \(\mcal B\)
    such that \(A = [T]_{\mcal B}\) is upper triangular.
    Hence, the characteristic polynomial is \(\det (xI-A) = \prod_{i=1}^n \big(x-(A)_{ii}\big)\).
    The result follows due to \Cref{th:charAndMinSameSol}.

    (\(\Leftarrow\))
    Suppose \(m(x) = \prod_{i=1}^k (x-c_i)^{r_i}\).
    We shall make use of \Cref{lem:TCondIsLinear} repeatedly over different choices of \(W\).
    With \(W = \{0\}\), we have \(\alpha \in V \setminus \{0\}\) such that \((T - d_1I) \alpha_1 = 0\)
    for some eigenvalue \(d_1\).
    Inductively define \(\alpha_i\) by:
    \begin{itemize}[nolistsep]
        \ii \(W_i = \spn \{\,\alpha_1, \cdots, \alpha_i\,\}\).
        \ii Thanks to \Cref{lem:TCondIsLinear}, take \(\alpha_{i+1} \in V \setminus W_i\)
            such that \((T-d_{i+1}I)\,\alpha_{i+1} \in W_i\)
            where \(d_{i+1}\) is an eigenvalue.
    \end{itemize}
    Then, by construction, \(\mcal B = \{\,\alpha_1, \cdots, \alpha_n\,\}\) is a basis for \(V\)
    and \([T]_{\mcal B}\) is an upper triangular matrix since
    \(T\alpha_{i+1} \in \spn \{\,\alpha_1, \cdots, \alpha_i\,\} + d_{i+1}\alpha_{i+1}\).
}

\cor{}{
    Let \(V\) be a \(n\)-dimensional vector space over an algebraically closed field \(F\).
    Then, every linear operator on \(V\) is triangulable.
}

\thm[diagIffMin]{}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(T \in L(V)\).
    Then, \(T\) is diagonalizable if and only if the minimal polynomial is
    \(m(x) = \prod_{i=1}^k (x-c_i)\) where \(c_1, \cdots, c_k\) are all the distinct
    eigenvalues of \(T\).
}
\pf{Proof}{
    (\(\Rightarrow\))
    By \Cref{th:diagTFAE} and \Cref{th:charAndMinSameSol}, we already have
    \(m(x) = \prod_{i=1}^k (x-c_i)^{e_i}\) where \(e_i \ge 1\).
    Now, we claim that \(S \triangleq \prod_{i=1}^k (T-c_iI) = 0\).

    From assumption, there exists a basis \(\{\,\alpha_1, \cdots, \alpha_n\,\}\)
    for \(V\) which consists of eigenvectors.
    Let \(\alpha_j\) corresponds to the eigenvalue \(c_{i(j)}\).
    Then, for each \(j \in [n]\), \((T-c_{i(j)}I) \alpha_j = 0\),
    i.e., \(S \alpha_j = 0\).
    Therefore, \(S = 0\).

    (\(\Leftarrow\))
    Let \(W\) be the subspace spanned by eigenvectors of \(T\).
    For the sake of contradiction, suppose \(W \subsetneq V\).
    As \(W\) is \(T\)-invariant, by \Cref{lem:TCondIsLinear},
    there exists \(\alpha \in V \setminus W\) and an eigenvalue \(c_j \in F\)
    such that \(\beta \triangleq (T-c_j)\,\alpha \in W\).
    % As \(\alpha\) is not an eigenvector, we have \(\beta \neq 0\).
    % As \(\beta \in W\), we may write
    % \(\beta = \sum_{i=1}^k \beta_i\) where \(\beta_i \in E_{c_i}\) for each \(i \in [k]\).
    % Note that \(f(T)\beta_i = f(c_i) \beta_i\) for each \(f \in F[x]\) and \(i \in [k]\).

    Write \(m(x) = (x-c_j)h(x)\) so \(h\) does not have \(x-c_j\) as a factor of it.
    As \(h(x) - h(c_j)\) has \(x=c_j\) as a root, \(h(x) - h(c_j) = (x-c_j)q(x)\)
    for some \(q\).
    Then, we have \[
        h(T)\alpha - h(c_j)\alpha = q(T) (T-c_jI)\alpha = q(T)\beta \in W
    \]
    since \(W\) is \(T\)-invariant.

    Moreover, \(0 = m(T)\alpha = (T-c_jI)h(T) \alpha\) and thus \(h(T) \alpha \in E_{c_j} \subseteq W\).
    This implies that \(h(c_j)\alpha \in W\) but \(\alpha \notin W\);
    thus \(h(c_j) = 0\), implying the multiplicity of \(x-c_j\) in the minimal polynomial.
}

\section{Simultaneous Triangulation and Diagonalization}

\dfn{Commuting Family of Linear Operators}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    A set of linear operators \(\mcal F\) is said to be
    a \textit{commuting family} of linear operators if
    \(T_1 T_2 = T_2 T_1\) for each \(T_1, T_2 \in \mcal F\).
}

\dfn{\(\mcal F\)-invariant}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    A subspace \(W\) of \(V\) is said to be \textit{\(\mcal F\)-invariant}
    if it is \(T\)-invariant for all \(T \in \mcal F\).
}

\mlemma[simulExtend]{}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Suppose \(\mcal F\) is a commuting family of triangulable linear operators
    on \(V\).
    Suppose a proper subspace \(W\) of \(V\) is \(\mcal F\)-invariant.
    Then, there exists \(\alpha \in V \setminus W\) such that
    \(\forall T \in \mcal F\), \(T \alpha \in \spn \{\,W, \alpha\,\}\).
}
\pf{Proof}{
    Suppose \(\{\,T_1, \cdots, T_r\,\}\) is a basis for the subspace spanned by \(\mcal F\).
    Note that \(\spn \mcal F\) is still a commuting family of triangulable linear operators.

    Let \(V_0 = V\). Construct \(V_1, \cdots, V_r\) and \(\beta_1, \cdots, \beta_r\)
    as follows. For each \(i \in [r]\),
    \begin{enumerate}[nolistsep, label=(\roman*)]
        \ii Let \(U_i = T_i \big|_{V_{i-1}}\). Then, \(U_i \in L(V_{i-1})\)
            by (iii)-(c).
        \ii Take \(\beta_i \in V_{i-1} \setminus W\) and \(c_i \in F\)
            such that \((U_i - c_iI) \beta_i \in W\).
            Their existence is guaranteed by \Cref{lem:TCondIsLinear} and (iii)-(b).
        \ii Let \(V_i \triangleq \{\,\beta \in V_{i-1} \mid (T_i - c_iI) \beta \in W \,\}\).
            Then, by construction, the following hold.
            \begin{enumerate}[nolistsep, label=(\alph*)]
                \ii \(\beta_i \in V_i \setminus W\)
                \ii \(W \subsetneq V_{i} \subseteq V_{i-1}\)
                \ii \(V_i\) is \(\mcal F\)-invariant as,
                    for each \(T \in \mcal F\) and \(\beta \in V_i\),
                    \((T_i-c_iI)(T \beta) = T(T_i-c_iI) \beta \in W\), i.e., \(T \beta \in V_i\).
            \end{enumerate}
    \end{enumerate}
    Then, \(\beta_r\) satisfies \(T_i \beta_r \in \spn \{\,W, \beta_r\,\}\)
    for each \(i \in [r]\).
}

\cor{}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(\mcal F\) be a commuting family of \textit{triangulable} linear operators on \(V\).
    Then, there exists a basis \(\mcal B\) for \(V\)
    such that \([T]_{\mcal B}\) is an \textit{upper triangular} matrix for all \(T \in \mcal F\).
}
\pf{Proof}{
    Take any \(\alpha_1 \in V\).
    Now, construct \(\alpha_2, \cdots, \alpha_n\) as following.
    For each \(i \in [n - 1]\),
    \begin{itemize}[nolistsep]
        \ii Let \(W_i \triangleq \spn \{\,\alpha_1, \cdots, \alpha_i\,\}\).
        \ii Take \(\alpha_{i+1} \in V \setminus W_i\) such that
            \(T \alpha_{i+1} \in \spn \{\,\alpha_1, \cdots, \alpha_{i+1}\,\}\)
            for each \(T \in \mcal F\).
            The existence is guaranteed by \Cref{lem:simulExtend}.
    \end{itemize}
    Then, \(\mcal B = \{\,\alpha_1;\cdots;\alpha_n\,\}\)
    is the ordered basis we are looking for.
}

\thm{}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(\mcal F\) be a commuting family of \textit{diagonalizable} linear operators on \(V\).
    Then, there exists a basis \(\mcal B\) for \(V\)
    such that \([T]_{\mcal B}\) is a \textit{diagonal} matrix for all \(T \in \mcal F\).
}
\pf{Proof}{
    We will apply the mathematical induction over \(\dim V\).
    If \(\dim V = 1\), there is nothing to prove. Hence, suppose
    the theorem holds for any finite-dimensional vector space \(V\) over \(F\)
    with dimension less than \(n\).

    If \(\mcal F\) only consists of multiples of identity, it is done.
    So we may assume the existence of \(T \in \mcal F\) which is not a multiple of identity.
    Let \(c_1, \cdots, c_k\) be its distinct characteristic values.
    For each \(i \in [k]\), let \(\mcal F_i \triangleq \big\{\,T\big|_{W_i} \in L(W_i, V) \colon T \in \mcal F\,\big\}\)
    where \(W_i\) is the eigenspace associated to \(c_i\).
    Then:
    \begin{enumerate}[nolistsep, label=(\roman*)]
        \ii As \(T\) is not a multiple of identity, \(k > 1\) and \(\dim W_i < n\).
        \ii As \(W_i\) is \(\mcal{F}\)-invariant, \(\mcal F_i \subseteq L(W_i)\).
        \ii For all \(T' \in \mcal F\), if \(m_i\) and \(m\) are minimal polynomials of \(T'\big|_{W_i}\) and \(T'\),
            \(m_i \mid m\) thanks to \Cref{lem:invariantMinAndChar}.
        \ii By (iii) and \Cref{th:diagIffMin},
            every linear operator in \(\mcal F_i\) is diagonalizable.
        \ii By (i), (iv), and the induction hypothesis, there exists a basis \(\mcal B_i\) for \(W_i\)
            that simultaneously diagonalize all linear operators in \(\mcal F_i\).
    \end{enumerate}
    Now, \(\mcal B = (\mcal B_1, \cdots, \mcal B_k)\) is an ordered basis for \(V\)
    due to \Cref{lem:eigenspaceIndep}, and \(\mcal B\) is the basis we are looking for.
}

\cor{}{
    Let \(V\) be a \(n\)-dimensional vector space over an \textit{algebraically closed field} \(F\).
    Let \(\mcal F\) be a commuting family of linear operators on \(V\).
    Then, there exists a basis \(\mcal B\) for \(V\)
    such that \([T]_{\mcal B}\) is a diagonal matrix for all \(T \in \mcal F\).
}

\section{Direct-Sum Decompositions}

\dfn{Independent Subspaces}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    We say subspaces \(W_1, \cdots, W_k\) of \(V\) are \textit{independent}
    if, whenever \(a_1 + \cdots + a_k = 0\) where \(a_i \in W_i\),
    \(a_i = 0\) for all \(i \in [k]\).
}

\dfn{Direct Sum}{
    Let \(V\) be a \(n\)-dimensional vector space over \(F\).
    Let \(W_1, \cdots, W_k\) be the finite number of subspaces of \(V\).
    Then, we say that the sum \(W = \sum_{i=1}^k W_i\) is \textit{direct}
    if \(W_1, \cdots, W_k\) are independent.
    We write \(W = W_1 \oplus \cdots \oplus W_k = \oplus_{i=1}^k W_i\)
    if the sum is direct.
}

\end{document}
